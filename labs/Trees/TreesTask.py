# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %%
#Импортируем дерево решений для классификации
import numpy as np
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
df = pd.read_csv('train.csv', sep=',')
print(df)


# %%
#Загрузите выборку из файла train.csv с помощью Pandas
#Оставьте в выборке 4 признака: класс пассажира (Pclass), цену билета (Fare), возраст пассажира (Age) и его пол (Sex)
#Обратите внимание, что признак Sex имеет строковые значения

#В данных есть пропущенные значения - например, для некоторых пассажиров неизвестен их возраст. Такие записи при чтении их в pandas принимают значение nan/
#Найдите все объекты, у которых есть пропущенные признаци, и удалите их из выборки


d_sample = pd.DataFrame(df,columns  = ['Pclass','Fare','Age','Sex'])

#выбор строк, отвечающих условию.
#d_sample_result = d_sample[d_sample['Age'].notna() & d_sample['Pclass'].notna() & d_sample['Fare'].notna() & d_sample['Sex'].notna()]


d_sample = d_sample.dropna()
d_sample['Sex'].replace(['male', 'female'], [1, 0], inplace=True)

print(d_sample)


# %%
#Выделите целевую переменную - она записана в столбце Survived
d_y = pd.DataFrame(df,columns = ['Survived','Age'])
d_y = d_y.dropna()
del d_y['Age']
print(d_y)


# %%
#как преобразовать в массив numpy для деревьев
d_sample = np.array(d_sample)
d_y = np.array(d_y)
print(d_sample)
print(d_y)


# %%
#Обучите решающее дерево с параметром random_state = 241 и 
#остальными параметрами по умолчанию

#Известно, что проблема изучения оптимального дерева решений является NP-полной по нескольким аспектам оптимальности и даже для простых понятий. Следовательно, практические алгоритмы обучения дерева решений основаны на эвристических алгоритмах, таких как жадный алгоритм, где на каждом узле принимаются локально оптимальные решения. Такие алгоритмы не могут гарантировать возврат глобально оптимального дерева решений. Это можно смягчить, обучая несколько деревьев учащемуся в ансамбле, где функции и образцы произвольно отбираются с заменой.
#Таким образом, в основном, недооптимальный жадный алгоритм повторяется несколько раз с использованием случайного выбора признаков и выборок (аналогичный метод используется в случайных лесах). Параметр random_state позволяет управлять этими случайными выборами.
#В документации по интерфейсу указано следующее:
#Если int, random_state – это семя, используемое генератором случайных чисел; Если экземпляр RandomState, random_state – генератор случайных чисел; Если None, генератор случайных чисел является экземпляром RandomState, используемым np.random.
#Таким образом, случайный алгоритм будет использоваться в любом случае. Передача любого значения (будь то конкретный int, например, 0 или экземпляр RandomState ) не изменит этого. Единственное обоснование для передачи значения int (0 или иначе) заключается в том, чтобы сделать результат согласованным между вызовами: если вы вызываете это с помощью random_state=0 (или любого другого значения), то каждый раз вы получите то же самое результат.

from sklearn.tree import tree
#Создаем модель дерева
clf = tree.DecisionTreeClassifier(random_state = 241)

#Обучаем, Создаёт классификатор дерева решений из обучающего набора (X, Y).
clf.fit(d_sample,d_y)


# %%
#Вычислите важности признаков и найдите два признака с наибольшей важностью. 
#Их названия будут ответами для данной задачи (в качестве ответа укажите названия признаков через запятую без пробелов)

x_labels = ['Pclass', 'Fare', 'Age', 'Sex']
importances = pd.Series(clf.feature_importances_, index=x_labels)
print(importances)
print(' '.join(importances.sort_values(ascending=False).head(2).index)) #сортировка Серии в порядке по убыванию, возращает первые два максимальных значения


# %%
from sklearn import tree
X = [[1,2], [3, 4], [5, 6]]#бучающие выборкиэ (3 элемента в обучающей выборке (3 строки) с двумя признаками)
Y = [0, 1,0]#содержит метки классов для обучающей выбороки
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
clf.predict([[2., 2.]])
tree.plot_tree(clf.fit(X,Y)) 


# %%
0.140005+0.303436+0.256046+0.300512


# %%
print(np.array([0, 1,0]))

